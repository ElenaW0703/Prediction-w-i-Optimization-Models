{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sta 663 - Statistical Computing and Computation - Midterm 2\n",
    "-----------\n",
    "Due Wednesday, April 20th by 5:00 pm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1 - `newton()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task completes the function of newton optimization. Firstly, convert all inputs to Tensors type and get the initial value for function. Before running newton method, check if all initials including objective and derivatives are finite, if one of them is not finite, then raise ValueError. Afterwards, creating while loop to iterate each step and set the convergent boolean value to ensure the final result is convergent. In this while loop, first, we should get the Jacobian and Hessian value for updated thetas, and then we could get step to update the further thetas and the output of function. Note that here we should check if hessian is positive definite: I define a new function `check_psd` to check, if it's positive definite, then return true, otherwise, false. Plus, if the hessian is not positive definite, then we should start by adding epsilon and keep multiplying epsilon by 10 until it is. Furthermore, we could move to the max_half loop inside maxit loop to get the new thetas and function results. Note here if reduction does not going well or the newton method is not converged, we should repeat half the step. If it's still fail to reduce the objective, then should raise 'step fails' error. If above is running well, we could store our new values in order to get the minimum at the end. Meanwhile, the most important task is to judge whether the gradient vector is close enough to zero by max(abs(g)) < (abs(f0)+fscale)*tol, and if it satisfies this condition, convergent will convert to True, otherwise, should raise the error after trying maxit. Finally, collecting all the necessary results to a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(theta, f, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 20):\n",
    "    \"\"\"\n",
    "    Return a dictionary containing:\n",
    "    f -- the value of the objective function at the minimum\n",
    "    theta -- the value of the parameters at the minimum\n",
    "    iter -- the number of iterations taken to reach the minimum\n",
    "    grad -- the gradient vector at the minimum\n",
    "    \n",
    "    Keyword arguments:\n",
    "    theta -- a vector of initial values for the optimization parameters\n",
    "    f -- the objective function to minimize\n",
    "    tol -- the convergence tolerance\n",
    "    fscale -- a rough estimate of the magnitude of f at the optimum \n",
    "    maxit -- the maximum number of Newton iterations to try before giving up\n",
    "    max_half -- the maximum number of times a step should be halved before concluding that the step has failed to improve the objective\n",
    "    \"\"\"\n",
    "        \n",
    "    # convert inputs to torch, create the list of inputs, and get the first input of function \n",
    "    theta_i = torch.tensor(theta, dtype = torch.double)\n",
    "    tol = torch.tensor(tol, dtype = torch.double)\n",
    "    fscale = torch.tensor(fscale, dtype = torch.double)\n",
    "    f_i = f(theta)\n",
    "    \n",
    "    f_i_list = []\n",
    "    theta_i_list = []\n",
    "    f_i_list.append(f_i)\n",
    "    theta_i_list.append(theta_i)\n",
    "    \n",
    "    y_0 = f(theta_i)\n",
    "    j_0 = torch.autograd.functional.jacobian(f, theta_i)\n",
    "    \n",
    "    # check if all initials, objective function and its derivative are finite\n",
    "    if torch.all(torch.isfinite(theta_i)) == True and torch.all(torch.isfinite(y_0)) == True and torch.all(torch.isfinite(j_0)) == True:\n",
    "        # iterate each step\n",
    "        i = 0\n",
    "        convergent = False\n",
    "        iterate = 1\n",
    "        while i < maxit and convergent == False:\n",
    "            j_i = torch.autograd.functional.jacobian(f, theta_i_list[-1])\n",
    "            h_i = torch.autograd.functional.hessian(f, theta_i_list[-1])\n",
    "            \n",
    "            # check if hessian is PD, if not, then perturbe Hessian\n",
    "            # define a function to check positive definite\n",
    "            def check_psd(A):\n",
    "                try:\n",
    "                    torch.linalg.cholesky(A)\n",
    "                    return True\n",
    "                except torch.linalg.LinAlgError:\n",
    "                    return False\n",
    "            k = 0\n",
    "            while (check_psd(h_i) == False):\n",
    "                epsilon = 10**(k)*torch.max(torch.abs(h_i))*1e-8*torch.eye(h_i.shape[1])\n",
    "                h_i = h_i + epsilon\n",
    "                # check if finite\n",
    "                if torch.any(torch.isinf(h_i)) == True:\n",
    "                    raise Exception(\"ValueError: Hessian is not finite\")\n",
    "                # add more epsilon\n",
    "                k = k+1\n",
    "                \n",
    "            # get step\n",
    "            step = -torch.linalg.solve(h_i, j_i)\n",
    "            \n",
    "            # iterate until max_half\n",
    "            for j in range(max_half):\n",
    "                theta_i_new = theta_i_list[-1] + step\n",
    "                f_i_new = f(theta_i_new)\n",
    "                \n",
    "                # check if reduce well or the newton method has converged, if not, repeatedly half the step\n",
    "                if (f_i_new < f_i_list[-1]) | (torch.max(torch.abs(j_i)) < (torch.abs(f_i_new)+fscale)*tol):\n",
    "                    break\n",
    "                step /= 2\n",
    "            \n",
    "            # check if step fails after max_half\n",
    "            if (f_i_new >= f_i_list[-1]) & ((torch.max(torch.abs(j_i)) > (torch.abs(f_i_new)+fscale)*tol)):\n",
    "                raise Exception(\"ValueError: step fails\") \n",
    "                  \n",
    "            # get new and store\n",
    "            f_i_list.append(f_i_new)\n",
    "            theta_i_list.append(theta_i_new)\n",
    "   \n",
    "            # check if current is convergent \n",
    "            if max(abs(j_i)) < (abs(f_i_new)+fscale)*tol:\n",
    "                convergent = True\n",
    "                break\n",
    "                \n",
    "            i += 1\n",
    "            iterate += 1\n",
    "            \n",
    "        # final result\n",
    "        grad = torch.autograd.functional.jacobian(f, theta_i_new)\n",
    "        result = {\n",
    "            \"f\": f_i_new,\n",
    "            \"theta\": theta_i_new,\n",
    "            \"iter\": iterate,\n",
    "            \"grad\": grad\n",
    "        }\n",
    "             \n",
    "        if i >= maxit and convergent == False:\n",
    "            raise Exception(\"ValueError: maxit is reached without convergence\")\n",
    "    else:\n",
    "        raise Exception(\"ValueError: the objective or derivatives are not finite at the initials\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Minimization examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Quadratic function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "def quadratic(theta):\n",
    "    x,y = theta\n",
    "    return x**2 - 2*x + 2*(y**2) + y + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 2,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([20,30], quadratic, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 2,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([-100,-200], quadratic, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 2,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([-1,1], quadratic, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rosenbrock's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "def Rosenbrock(theta):\n",
    "    x,y = theta\n",
    "    return 10 * (y - x**2)**2 + (1 - x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(0., dtype=torch.float64),\n",
       " 'theta': tensor([1., 1.], dtype=torch.float64),\n",
       " 'iter': 6,\n",
       " 'grad': tensor([-0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([0.3,0.5], Rosenbrock, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(0., dtype=torch.float64),\n",
       " 'theta': tensor([1., 1.], dtype=torch.float64),\n",
       " 'iter': 38,\n",
       " 'grad': tensor([-0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([20,30], Rosenbrock, tol = 1e-8, fscale = 1.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(0., dtype=torch.float64),\n",
       " 'theta': tensor([1., 1.], dtype=torch.float64),\n",
       " 'iter': 157,\n",
       " 'grad': tensor([-0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([200,300], Rosenbrock, tol = 1e-8, fscale = 1.0, maxit = 200, max_half = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Poisson regression likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "   0.11, -0.06, -0.96, -0.48, -0.59, -0.42, -0.15,  1.14, 0.94, \n",
    "  -0.86, -0.08,  1.00, -2.01,  2.17, -0.20,  0.82, -0.13, 0.26, \n",
    "   0.22,  1.05\n",
    "]\n",
    "\n",
    "y = [4, 2, 4, 1, 1, 3, 4, 5, 7, 3, 5, 7, 0, 4, 2, 7, 3, 3, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "def PoissonRegression(theta):\n",
    "    beta_0, beta_1 = theta\n",
    "    lambda_i = beta_0 + beta_1 * torch.tensor(x)\n",
    "    likelihood = torch.sum(torch.tensor(y)*lambda_i-torch.exp(lambda_i)-torch.log(torch.tensor([torch.jit._builtins.math.factorial(i) for i in y])))\n",
    "    return -likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 60,\n",
       " 'grad': tensor([-2.3842e-06, -1.1921e-06], dtype=torch.float64)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([15,20], PoissonRegression, tol = 1e-4, fscale = 30.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 11,\n",
       " 'grad': tensor([-2.3842e-06,  5.9605e-07], dtype=torch.float64)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([1,4], PoissonRegression, tol = 1e-4, fscale = 30.0, maxit = 100, max_half = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 23,\n",
       " 'grad': tensor([-2.1458e-06,  1.1921e-07], dtype=torch.float64)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([7,7], PoissonRegression, tol = 1e-4, fscale = 30.0, maxit = 100, max_half = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
